{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import cv2\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Image as _Imgdis\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory variables\n",
    "DATADIR = r\".\" # The directory to SAFE and UNSAFE PFM/PNG images\n",
    "STATES_PNG = [\"Safe_Collection\", \"Unsafe_Collection\"] #Sub-directories for png files\n",
    "SAFE_IMG_PNG = STATES_PNG[0] \n",
    "UNSAFE_IMG_PNG = STATES_PNG[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDF():\n",
    "    #dataToAR()\n",
    "\n",
    "    #appends safe & unsafe images to the appropriate lists to later use as labels\n",
    "    imageSafeList = pd.Series()\n",
    "    imageUnsafeList = pd.Series()\n",
    "    images_df = pd.DataFrame(columns=[\"image_Data\", \"State\"])\n",
    "    \n",
    "    \n",
    "    for file in os.listdir(DATADIR + '\\\\' + SAFE_IMG_PNG):\n",
    "        try:\n",
    "            \n",
    "            frame = cv2.imread(DATADIR + '\\\\' + SAFE_IMG_PNG + '\\\\' + file)\n",
    "            frame = cv2.resize(frame, (50, 50))\n",
    "            images_df.loc[file] = [frame, \"Safe\"]\n",
    "    \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    for file in os.listdir(DATADIR + '\\\\' + UNSAFE_IMG_PNG):\n",
    "        try:\n",
    "            frame = cv2.imread(DATADIR + '\\\\' + UNSAFE_IMG_PNG + '\\\\' + file)\n",
    "            frame = cv2.resize(frame, (50, 50))\n",
    "            images_df.loc[file] = [frame, \"Unsafe\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return images_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def dataPrep(image_df): \\n    folder_safe = DATADIR + \\'\\\\\\'+ STATES_PNG[0]\\n    folder_unsafe = DATADIR + \\'\\\\\\'+ STATES_PNG[1]\\n    \\n    safeFiles = [x for x in os.listdir(folder_safe) if os.path.isfile(os.path.join(folder_safe, x))]\\n    unsafeFiles = [y for y in os.listdir(folder_unsafe) if os.path.isfile(os.path.join(folder_unsafe, y))]\\n\\n    print(\"Working with {0} images\".format(len(safeFiles)))\\n    print(\"Image examples: \")\\n    \\n    print(type(safeFiles))\\n    \\n    for i in range(40, 42):\\n        print(safeFiles[i])\\n        display(_Imgdis(filename=folder_safe + \"/\" + safeFiles[i], width=240, height=320))\\n    \\n    train_data = []\\n    y_train = []\\n    for file in safeFiles:\\n        frame = cv2.imread(DATADIR + \\'\\\\\\' + STATES_PNG[0] + \\'\\\\\\' + file)\\n        im = Image.open(frame)\\n        print(im.size)\\n        train_data.append(file)\\n        y_train.append(\"safe\")\\n        \\n        \\n    print(\"Files in train_data: %d\" % len(train_data))\\n    \\n    channels = 3\\n    nb_classes = 1\\n    \\n    dataset = np.ndarray(shape=(len(train_data), channels, 50, 50), dtype=np.float32)\\n    \\n    i = 0\\n    for file in train_data:\\n        img = load_img(folder_safe+\"/\"+file)\\n        img.thumbnail((50, 50))\\n        #numpy array conversion\\n        x = img_to_array(img)\\n        x.reshape((3,120,160))\\n        #normalize\\n        x = (x - 128.0) / 128.0\\n        dataset[i] = x\\n        i += 1\\n        if i % 250 == 0:\\n            print(\"%d images to array\" % i)\\n    print(\"All images to array!\")\\n    splitData(y_train, x_data)\\n    '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def dataPrep(image_df): \n",
    "    folder_safe = DATADIR + '\\\\'+ STATES_PNG[0]\n",
    "    folder_unsafe = DATADIR + '\\\\'+ STATES_PNG[1]\n",
    "    \n",
    "    safeFiles = [x for x in os.listdir(folder_safe) if os.path.isfile(os.path.join(folder_safe, x))]\n",
    "    unsafeFiles = [y for y in os.listdir(folder_unsafe) if os.path.isfile(os.path.join(folder_unsafe, y))]\n",
    "\n",
    "    print(\"Working with {0} images\".format(len(safeFiles)))\n",
    "    print(\"Image examples: \")\n",
    "    \n",
    "    print(type(safeFiles))\n",
    "    \n",
    "    for i in range(40, 42):\n",
    "        print(safeFiles[i])\n",
    "        display(_Imgdis(filename=folder_safe + \"/\" + safeFiles[i], width=240, height=320))\n",
    "    \n",
    "    train_data = []\n",
    "    y_train = []\n",
    "    for file in safeFiles:\n",
    "        frame = cv2.imread(DATADIR + '\\\\' + STATES_PNG[0] + '\\\\' + file)\n",
    "        im = Image.open(frame)\n",
    "        print(im.size)\n",
    "        train_data.append(file)\n",
    "        y_train.append(\"safe\")\n",
    "        \n",
    "        \n",
    "    print(\"Files in train_data: %d\" % len(train_data))\n",
    "    \n",
    "    channels = 3\n",
    "    nb_classes = 1\n",
    "    \n",
    "    dataset = np.ndarray(shape=(len(train_data), channels, 50, 50), dtype=np.float32)\n",
    "    \n",
    "    i = 0\n",
    "    for file in train_data:\n",
    "        img = load_img(folder_safe+\"/\"+file)\n",
    "        img.thumbnail((50, 50))\n",
    "        #numpy array conversion\n",
    "        x = img_to_array(img)\n",
    "        x.reshape((3,120,160))\n",
    "        #normalize\n",
    "        x = (x - 128.0) / 128.0\n",
    "        dataset[i] = x\n",
    "        i += 1\n",
    "        if i % 250 == 0:\n",
    "            print(\"%d images to array\" % i)\n",
    "    print(\"All images to array!\")\n",
    "    splitData(y_train, x_data)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSplit(dataframe):\n",
    "    Y = dataframe['State'].values.reshape(dataframe.shape[0],1)\n",
    "    dataframe = dataframe.drop('State', 1)\n",
    "    dataset = dataframe.values\n",
    "    #print(dataset)\n",
    "    X = dataset[:,0:dataset.shape[1]]\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "    \n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    CNN_training(X_train, X_test,Y_train,Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_training(x_train, x_test, y_train,y_test):\n",
    "    \n",
    "    pickle_out = open(\"X.pickle\", \"wb\")\n",
    "    pickle.dump(x_train,pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "    pickle_out = open(\"y.pickle\", \"wb\")\n",
    "    pickle.dump(y_train,pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "    pickle_in = open(\"X.pickle\", \"rb\")\n",
    "    X_train = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"y.pickle\", \"rb\")\n",
    "    y_train = pickle.load(pickle_in)\n",
    "\n",
    "    x_train = x_train/255.0 #normalize\n",
    "    x_train = np.array(x_train)\n",
    "    \n",
    "    dense_layers = [0]\n",
    "    layer_sizes = [64]\n",
    "    conv_layers = [1]\n",
    "\n",
    "    for dense_layer in dense_layers:\n",
    "        for layer_size in layer_sizes:\n",
    "            for conv_layer in conv_layers:\n",
    "                NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "                print(NAME)\n",
    "                model = Sequential() #feed-forward network\n",
    "                model.add(Conv2D(layer_size, (3, 3), input_shape=x_train.shape(50, 50, 1)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            for _ in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                            optimizer='adam',\n",
    "                            metrics=['accuracy'],\n",
    "                            )\n",
    "\n",
    "            model.fit(x_train, y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=10,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[tensorboard])\n",
    "\n",
    "    model.save('CNN_tester.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "1-conv-64-nodes-0-dense-1574312623\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-ea302f38160a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimages_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdataSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-ea302f38160a>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#Here we are building a dataframe of our images and their labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimages_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdataSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-4e17819b4ce1>\u001b[0m in \u001b[0;36mdataSplit\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mCNN_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-344eafa08427>\u001b[0m in \u001b[0;36mCNN_training\u001b[1;34m(x_train, x_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#feed-forward network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #Here we are building a dataframe of our images and their labels\n",
    "    images_df = buildDF()\n",
    "    dataSplit(images_df)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
